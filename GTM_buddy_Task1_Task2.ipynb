{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_MXdF1_BEtVw"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Data Preparation & Multi-Label Text Classification\n",
        "### 1. Generate the synthetic data for calls_dataset.csv and domain_knowledge.json."
      ],
      "metadata": {
        "id": "wrFfOgKhAnXO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6QRBvzq2v1k",
        "outputId": "77189e28-5c37-4666-e02f-9eaacc78d145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synthetic dataset saved as calls_dataset.csv.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Define categories and sample data\n",
        "categories = [\"Objection\", \"Pricing Discussion\", \"Security\", \"Competition\"]\n",
        "sample_snippets = [\n",
        "    \"We love the analytics, but CompetitorX has a cheaper subscription.\",\n",
        "    \"Our compliance team is worried about data handling. Are you SOC2 certified?\",\n",
        "    \"Can you offer any discounts on your current pricing model?\",\n",
        "    \"CompetitorY provides similar features at a lower cost.\",\n",
        "    \"We need more information about the AI engine before proceeding.\",\n",
        "]\n",
        "\n",
        "# Generate synthetic dataset\n",
        "data = []\n",
        "for i in range(1, 201):  # Generate 200 rows\n",
        "    snippet = random.choice(sample_snippets)\n",
        "    labels = random.sample(categories, random.randint(1, len(categories)))  # Random labels\n",
        "    data.append({\"id\": i, \"text_snippet\": snippet, \"labels\": \", \".join(labels)})\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv(\"calls_dataset.csv\", index=False)\n",
        "print(\"Synthetic dataset saved as calls_dataset.csv.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lj_AoRWjX49E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPA7R6Bc28K9",
        "outputId": "61975ea6-b3ef-4c96-e9cb-d8da83e4d37e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id                                       text_snippet  \\\n",
            "0   1  We need more information about the AI engine b...   \n",
            "1   2  We love the analytics, but CompetitorX has a c...   \n",
            "2   3  We need more information about the AI engine b...   \n",
            "3   4  Can you offer any discounts on your current pr...   \n",
            "4   5  Our compliance team is worried about data hand...   \n",
            "\n",
            "                                              labels  \n",
            "0                             Competition, Objection  \n",
            "1          Pricing Discussion, Competition, Security  \n",
            "2                                          Objection  \n",
            "3                                          Objection  \n",
            "4  Objection, Security, Pricing Discussion, Compe...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  2. Clean & Preprocess the text: Removing stop words."
      ],
      "metadata": {
        "id": "aK0ID1-jA6JJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u08x5T3O3lgo",
        "outputId": "86d7a856-a506-4230-d422-0219854c86a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('wordnet') # Download the wordnet dataset\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"calls_dataset.csv\")\n",
        "\n",
        "# Text preprocessing function\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
        "    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing\n",
        "df[\"cleaned_text\"] = df[\"text_snippet\"].apply(preprocess_text)\n",
        "\n",
        "# Print the cleaned dataset\n",
        "print(df[[\"text_snippet\", \"cleaned_text\"]].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zy4uAARL3CMr",
        "outputId": "c37ea9ef-b553-420a-9ef4-bc52da1943c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        text_snippet  \\\n",
            "0  Can you offer any discounts on your current pr...   \n",
            "1  We need more information about the AI engine b...   \n",
            "2  Can you offer any discounts on your current pr...   \n",
            "3  We love the analytics, but CompetitorX has a c...   \n",
            "4  Can you offer any discounts on your current pr...   \n",
            "\n",
            "                                      cleaned_text  \n",
            "0             offer discount current pricing model  \n",
            "1            need information ai engine proceeding  \n",
            "2             offer discount current pricing model  \n",
            "3  love analytics competitorx cheaper subscription  \n",
            "4             offer discount current pricing model  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Split your data into training and validation/test."
      ],
      "metadata": {
        "id": "TQYUYxKg4IX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train and test sets\n",
        "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_data.to_csv(\"train_data.csv\", index=False)\n",
        "test_data.to_csv(\"test_data.csv\", index=False)\n",
        "print(\"Train and test datasets created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJUPRozy4S-F",
        "outputId": "36ba51c0-2496-4aef-8cab-cdb035525379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train and test datasets created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Choose a suitable multi-label classification approach: Logistic regression has been choosen"
      ],
      "metadata": {
        "id": "7csVVtqz4kuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.Training the model:"
      ],
      "metadata": {
        "id": "XAe5eDrt43k9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Vectorize the text\n",
        "vectorizer = TfidfVectorizer(max_features=500)\n",
        "X_train = vectorizer.fit_transform(train_data[\"cleaned_text\"])\n",
        "X_test = vectorizer.transform(test_data[\"cleaned_text\"])\n",
        "\n",
        "# Encode labels\n",
        "mlb = MultiLabelBinarizer()\n",
        "y_train = mlb.fit_transform(train_data[\"labels\"].str.split(\", \"))\n",
        "y_test = mlb.transform(test_data[\"labels\"].str.split(\", \"))\n",
        "\n",
        "# Train the model\n",
        "classifier = OneVsRestClassifier(LogisticRegression())\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = classifier.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=mlb.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwESphsZ4ejD",
        "outputId": "17dfa7bd-82be-4e10-99d3-0383908be661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    precision    recall  f1-score   support\n",
            "\n",
            "       Competition       0.70      1.00      0.82        28\n",
            "         Objection       0.70      1.00      0.82        28\n",
            "Pricing Discussion       0.36      0.44      0.40        18\n",
            "          Security       0.68      1.00      0.81        27\n",
            "\n",
            "         micro avg       0.64      0.90      0.75       101\n",
            "         macro avg       0.61      0.86      0.71       101\n",
            "      weighted avg       0.63      0.90      0.74       101\n",
            "       samples avg       0.65      0.93      0.72       101\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### with hyperparameter tuning"
      ],
      "metadata": {
        "id": "S7R8S_dC5z9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Vectorize the text with TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=500)\n",
        "X_train = vectorizer.fit_transform(train_data[\"cleaned_text\"])\n",
        "X_test = vectorizer.transform(test_data[\"cleaned_text\"])\n",
        "\n",
        "# Encode labels\n",
        "mlb = MultiLabelBinarizer()\n",
        "y_train = mlb.fit_transform(train_data[\"labels\"].str.split(\", \"))\n",
        "y_test = mlb.transform(test_data[\"labels\"].str.split(\", \"))\n",
        "\n",
        "# Define parameter grid for Logistic Regression\n",
        "param_grid = {\n",
        "    \"estimator__C\": [0.01, 0.1, 1, 10],       # Regularization strength\n",
        "    \"estimator__penalty\": [\"l2\"],             # Regularization type\n",
        "    \"estimator__solver\": [\"lbfgs\", \"saga\"],   # Solvers\n",
        "}\n",
        "\n",
        "# Set up Logistic Regression and OneVsRestClassifier\n",
        "logistic = LogisticRegression(max_iter=1000)\n",
        "classifier = OneVsRestClassifier(logistic)\n",
        "\n",
        "# Perform Grid Search\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=classifier,\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"f1_micro\",       # Use f1_micro as the scoring metric\n",
        "    cv=3,                     # Cross-validation folds\n",
        "    verbose=1,\n",
        "    n_jobs=-1                 # Parallel processing\n",
        ")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=mlb.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1O0ScahY5b8B",
        "outputId": "7c7ad712-51dd-4055-e8dd-010d0a527ee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
            "Best parameters: {'estimator__C': 0.01, 'estimator__penalty': 'l2', 'estimator__solver': 'lbfgs'}\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "       Competition       0.68      1.00      0.81        27\n",
            "         Objection       0.60      1.00      0.75        24\n",
            "Pricing Discussion       0.60      1.00      0.75        24\n",
            "          Security       0.65      1.00      0.79        26\n",
            "\n",
            "         micro avg       0.63      1.00      0.77       101\n",
            "         macro avg       0.63      1.00      0.77       101\n",
            "      weighted avg       0.63      1.00      0.77       101\n",
            "       samples avg       0.63      1.00      0.74       101\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.b Cross Validation: used K- Fold Cross Validation Technique"
      ],
      "metadata": {
        "id": "qIi81TU96xcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import numpy as np\n",
        "\n",
        "# Encode labels\n",
        "mlb = MultiLabelBinarizer()\n",
        "y = mlb.fit_transform(df[\"labels\"].str.split(\", \"))\n",
        "\n",
        "# Vectorize the text\n",
        "vectorizer = TfidfVectorizer(max_features=500, ngram_range=(1, 2))\n",
        "\n",
        "# Convert text data into features\n",
        "X = vectorizer.fit_transform(df[\"cleaned_text\"])\n",
        "\n",
        "# Define the model\n",
        "model = OneVsRestClassifier(LogisticRegression(max_iter=1000, C=1.0))\n",
        "\n",
        "# K-Fold Cross-Validation\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold = 1\n",
        "f1_scores = []\n",
        "\n",
        "for train_index, test_index in kfold.split(X, y):\n",
        "    # Split data into training and testing sets for this fold\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate performance using F1-score\n",
        "    fold_f1 = f1_score(y_test, y_pred, average=\"micro\")\n",
        "    f1_scores.append(fold_f1)\n",
        "\n",
        "    print(f\"Fold {fold} F1-Score: {fold_f1:.4f}\")\n",
        "    fold += 1\n",
        "\n",
        "# Display overall results\n",
        "print(\"\\nK-Fold Cross-Validation Results\")\n",
        "print(f\"Mean F1-Score: {np.mean(f1_scores):.4f}\")\n",
        "print(f\"Standard Deviation: {np.std(f1_scores):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axmRG_5A7AL1",
        "outputId": "aa14f05f-bddb-4d2a-d4a1-5da8d85e91c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 F1-Score: 0.7490\n",
            "Fold 2 F1-Score: 0.7131\n",
            "Fold 3 F1-Score: 0.7325\n",
            "Fold 4 F1-Score: 0.6987\n",
            "Fold 5 F1-Score: 0.7750\n",
            "\n",
            "K-Fold Cross-Validation Results\n",
            "Mean F1-Score: 0.7337\n",
            "Standard Deviation: 0.0268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Evaluate your model:"
      ],
      "metadata": {
        "id": "9D0Tm58K7KOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a.  precision, recall, F1-score per label."
      ],
      "metadata": {
        "id": "TY5WG7TR8Hx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "y_pred = classifier.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=mlb.classes_))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoFox49B_PRj",
        "outputId": "9bca0d62-26cb-4f64-e514-acabca514457"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    precision    recall  f1-score   support\n",
            "\n",
            "       Competition       0.68      1.00      0.81        27\n",
            "         Objection       0.61      0.71      0.65        24\n",
            "Pricing Discussion       0.62      0.88      0.72        24\n",
            "          Security       0.65      1.00      0.79        26\n",
            "\n",
            "         micro avg       0.64      0.90      0.75       101\n",
            "         macro avg       0.64      0.90      0.74       101\n",
            "      weighted avg       0.64      0.90      0.75       101\n",
            "       samples avg       0.64      0.89      0.71       101\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b. a confusion matrix"
      ],
      "metadata": {
        "id": "P2VmEjic8O8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Print confusion matrix for each label\n",
        "print(\"Confusion Matrix Results:\\n\")\n",
        "for i, label in enumerate(mlb.classes_):\n",
        "    cm = confusion_matrix(y_test[:, i], y_pred[:, i])  # Confusion matrix for each label\n",
        "    print(f\"Confusion Matrix for Label: {label}\")\n",
        "    print(cm)\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEVjaKaoAJS-",
        "outputId": "726395c9-c353-4d33-cdab-a1b3be617ca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix Results:\n",
            "\n",
            "Confusion Matrix for Label: Competition\n",
            "[[ 0 13]\n",
            " [ 0 27]]\n",
            "\n",
            "\n",
            "Confusion Matrix for Label: Objection\n",
            "[[ 5 11]\n",
            " [ 7 17]]\n",
            "\n",
            "\n",
            "Confusion Matrix for Label: Pricing Discussion\n",
            "[[ 3 13]\n",
            " [ 3 21]]\n",
            "\n",
            "\n",
            "Confusion Matrix for Label: Security\n",
            "[[ 0 14]\n",
            " [ 0 26]]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Entity/Keyword Extraction with a Domain Knowledge Base"
      ],
      "metadata": {
        "id": "DDALKmdlBLuy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Dictionary Lookup:  Using domain_knowledge.json to search for known competitor names, product features, or pricing keywords in the text."
      ],
      "metadata": {
        "id": "mEqLjhoABYCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"calls_dataset.csv\")\n",
        "\n",
        "# Load the domain knowledge base\n",
        "domain_knowledge = {\n",
        "    \"competitors\": [\"CompetitorX\", \"CompetitorY\", \"CompetitorZ\"],\n",
        "    \"features\": [\"analytics\", \"AI engine\", \"data pipeline\"],\n",
        "    \"pricing_keywords\": [\"discount\", \"renewal cost\", \"budget\", \"pricing model\"]\n",
        "}\n"
      ],
      "metadata": {
        "id": "XF305f0iBfCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## 2. NER or Advanced Extraction:"
      ],
      "metadata": {
        "id": "0Y1HT5B7CZxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### a. A simple rule-based approach (regex or keyword expansion)."
      ],
      "metadata": {
        "id": "0aT0_9nVC7pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract entities using the dictionary\n",
        "def dictionary_lookup(text, knowledge_base):\n",
        "    extracted_entities = {\n",
        "        \"competitors\": [],\n",
        "        \"features\": [],\n",
        "        \"pricing_keywords\": []\n",
        "    }\n",
        "    for category, keywords in knowledge_base.items():\n",
        "        for keyword in keywords:\n",
        "            if keyword.lower() in text.lower():\n",
        "                extracted_entities[category].append(keyword)\n",
        "    return extracted_entities\n",
        "\n",
        "# Apply dictionary lookup to the dataset\n",
        "df[\"dictionary_entities\"] = df[\"text_snippet\"].apply(lambda x: dictionary_lookup(x, domain_knowledge))\n",
        "print(df[[\"text_snippet\", \"dictionary_entities\"]].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWRnHolBBuzt",
        "outputId": "bb1e1d6b-eb91-4b6b-ec05-0949dd1b8394"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        text_snippet  \\\n",
            "0  We need more information about the AI engine b...   \n",
            "1  We love the analytics, but CompetitorX has a c...   \n",
            "2  We need more information about the AI engine b...   \n",
            "3  Can you offer any discounts on your current pr...   \n",
            "4  Our compliance team is worried about data hand...   \n",
            "\n",
            "                                 dictionary_entities  \n",
            "0  {'competitors': [], 'features': ['AI engine'],...  \n",
            "1  {'competitors': ['CompetitorX'], 'features': [...  \n",
            "2  {'competitors': [], 'features': ['AI engine'],...  \n",
            "3  {'competitors': [], 'features': [], 'pricing_k...  \n",
            "4  {'competitors': [], 'features': [], 'pricing_k...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def regex_lookup(text, knowledge_base):\n",
        "    extracted_entities = {\n",
        "        \"competitors\": [],\n",
        "        \"features\": [],\n",
        "        \"pricing_keywords\": []\n",
        "    }\n",
        "    for category, keywords in knowledge_base.items():\n",
        "        for keyword in keywords:\n",
        "            pattern = r'\\b' + re.escape(keyword) + r'\\b'  # Match exact whole word\n",
        "            if re.search(pattern, text, re.IGNORECASE):\n",
        "                extracted_entities[category].append(keyword)\n",
        "    return extracted_entities\n",
        "\n",
        "# Apply regex lookup to the dataset\n",
        "df[\"regex_entities\"] = df[\"text_snippet\"].apply(lambda x: regex_lookup(x, domain_knowledge))\n",
        "print(df[[\"text_snippet\", \"regex_entities\"]].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azbFZxI9C24F",
        "outputId": "03b10285-a2ff-4b11-b31f-050c6c0eeaf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        text_snippet  \\\n",
            "0  We need more information about the AI engine b...   \n",
            "1  We love the analytics, but CompetitorX has a c...   \n",
            "2  We need more information about the AI engine b...   \n",
            "3  Can you offer any discounts on your current pr...   \n",
            "4  Our compliance team is worried about data hand...   \n",
            "\n",
            "                                      regex_entities  \n",
            "0  {'competitors': [], 'features': ['AI engine'],...  \n",
            "1  {'competitors': ['CompetitorX'], 'features': [...  \n",
            "2  {'competitors': [], 'features': ['AI engine'],...  \n",
            "3  {'competitors': [], 'features': [], 'pricing_k...  \n",
            "4  {'competitors': [], 'features': [], 'pricing_k...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b.  A pre-trained NER model : SPACY to extract entities."
      ],
      "metadata": {
        "id": "YRl7W1vcC_4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHVRL0aGB4HA",
        "outputId": "0878ca48-e8f0-4a2d-fa70-64ca52d77dab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to extract named entities\n",
        "def ner_extraction(text):\n",
        "    doc = nlp(text)\n",
        "    entities = {\"ORG\": [], \"PRODUCT\": []}  # Customize based on domain\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"ORG\":  # Organization names\n",
        "            entities[\"ORG\"].append(ent.text)\n",
        "        elif ent.label_ in [\"PRODUCT\", \"NORP\"]:  # Product features or related keywords\n",
        "            entities[\"PRODUCT\"].append(ent.text)\n",
        "    return entities\n",
        "\n",
        "# Apply NER extraction to the dataset\n",
        "df[\"ner_entities\"] = df[\"text_snippet\"].apply(ner_extraction)\n",
        "print(df[[\"text_snippet\", \"ner_entities\"]].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdognC-YCAvB",
        "outputId": "acd77876-01c8-436c-b2d8-73f06c5df712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        text_snippet  \\\n",
            "0  We need more information about the AI engine b...   \n",
            "1  We love the analytics, but CompetitorX has a c...   \n",
            "2  We need more information about the AI engine b...   \n",
            "3  Can you offer any discounts on your current pr...   \n",
            "4  Our compliance team is worried about data hand...   \n",
            "\n",
            "                     ner_entities  \n",
            "0  {'ORG': ['AI'], 'PRODUCT': []}  \n",
            "1      {'ORG': [], 'PRODUCT': []}  \n",
            "2  {'ORG': ['AI'], 'PRODUCT': []}  \n",
            "3      {'ORG': [], 'PRODUCT': []}  \n",
            "4      {'ORG': [], 'PRODUCT': []}  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Combine both approaches to produce a final set of extracted entities."
      ],
      "metadata": {
        "id": "y38hI-yODS7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine both approaches\n",
        "def combine_entities(dict_entities, ner_entities):\n",
        "    combined = dict_entities.copy()\n",
        "    for category, items in ner_entities.items():\n",
        "        if category == \"ORG\":\n",
        "            combined[\"competitors\"].extend(items)  # Map NER category to domain knowledge\n",
        "        elif category == \"PRODUCT\":\n",
        "            combined[\"features\"].extend(items)\n",
        "    return {k: list(set(v)) for k, v in combined.items()}  # Remove duplicates\n",
        "\n",
        "# Apply the combined entity extraction\n",
        "df[\"combined_entities\"] = df.apply(lambda x: combine_entities(x[\"dictionary_entities\"], x[\"ner_entities\"]), axis=1)\n",
        "print(df[[\"text_snippet\", \"combined_entities\"]].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIDyJVhGCQwL",
        "outputId": "b0aa908f-5c94-4302-da5d-92ca4382e659"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        text_snippet  \\\n",
            "0  We need more information about the AI engine b...   \n",
            "1  We love the analytics, but CompetitorX has a c...   \n",
            "2  We need more information about the AI engine b...   \n",
            "3  Can you offer any discounts on your current pr...   \n",
            "4  Our compliance team is worried about data hand...   \n",
            "\n",
            "                                   combined_entities  \n",
            "0  {'competitors': ['AI'], 'features': ['AI engin...  \n",
            "1  {'competitors': ['CompetitorX'], 'features': [...  \n",
            "2  {'competitors': ['AI'], 'features': ['AI engin...  \n",
            "3  {'competitors': [], 'features': [], 'pricing_k...  \n",
            "4  {'competitors': [], 'features': [], 'pricing_k...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Assume 'true_entities' is a column containing ground truth\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "# Binarize extracted and true entities for each category\n",
        "# This part depends on having labeled ground truth data\n",
        "# Adapt to match available annotations\n"
      ],
      "metadata": {
        "id": "BtcXZS5mDb2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Output the extracted entities for each snippet in a structured format."
      ],
      "metadata": {
        "id": "_MXdF1_BEtVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results to CSV\n",
        "df[[\"id\", \"text_snippet\", \"combined_entities\"]].to_csv(\"extracted_entities.csv\", index=False)\n",
        "print(\"Extracted entities saved to extracted_entities.csv.\")\n",
        "\n",
        "print(df[[\"id\", \"text_snippet\", \"combined_entities\"]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKC3xigNDfl7",
        "outputId": "4e589a53-7ae6-4fed-d3ec-47f41fcbb294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted entities saved to extracted_entities.csv.\n",
            "      id                                       text_snippet  \\\n",
            "0      1  We need more information about the AI engine b...   \n",
            "1      2  We love the analytics, but CompetitorX has a c...   \n",
            "2      3  We need more information about the AI engine b...   \n",
            "3      4  Can you offer any discounts on your current pr...   \n",
            "4      5  Our compliance team is worried about data hand...   \n",
            "..   ...                                                ...   \n",
            "195  196  CompetitorY provides similar features at a low...   \n",
            "196  197  CompetitorY provides similar features at a low...   \n",
            "197  198  We love the analytics, but CompetitorX has a c...   \n",
            "198  199  Our compliance team is worried about data hand...   \n",
            "199  200  CompetitorY provides similar features at a low...   \n",
            "\n",
            "                                     combined_entities  \n",
            "0    {'competitors': ['AI'], 'features': ['AI engin...  \n",
            "1    {'competitors': ['CompetitorX'], 'features': [...  \n",
            "2    {'competitors': ['AI'], 'features': ['AI engin...  \n",
            "3    {'competitors': [], 'features': [], 'pricing_k...  \n",
            "4    {'competitors': [], 'features': [], 'pricing_k...  \n",
            "..                                                 ...  \n",
            "195  {'competitors': ['CompetitorY'], 'features': [...  \n",
            "196  {'competitors': ['CompetitorY'], 'features': [...  \n",
            "197  {'competitors': ['CompetitorX'], 'features': [...  \n",
            "198  {'competitors': [], 'features': [], 'pricing_k...  \n",
            "199  {'competitors': ['CompetitorY'], 'features': [...  \n",
            "\n",
            "[200 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pkl\n"
      ],
      "metadata": {
        "id": "XwNFVfwYX9Qg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "# Save the trained model (classifier)\n",
        "with open(\"model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(classifier, f)\n",
        "print(\"Model saved as model.pkl\")\n",
        "\n",
        "# Save the vectorizer (important for text preprocessing)\n",
        "with open(\"vectorizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(vectorizer, f)\n",
        "print(\"Vectorizer saved as vectorizer.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ske58lvHX8RX",
        "outputId": "abd3e212-b7d8-45fd-f87a-8aa3b0783594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as model.pkl\n",
            "Vectorizer saved as vectorizer.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "\n",
        "projectabspathname = os.path.abspath('projectname.pickle')\n",
        "print(projectabspathname)\n",
        "projectname = 'GTM_buddy_Task1.ipynb'\n",
        "projectpickle = open(str(projectabspathname),'wb')\n",
        "pickle.dump(projectname, projectpickle)\n",
        "projectpickle.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8BtQO8qczFM",
        "outputId": "8b07b2e8-b842-4c7d-8a03-68fb5636eafc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/projectname.pickle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('/content/projectname.pickle')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Q-SGnKGNdPqt",
        "outputId": "50df4bc2-8cd4-4a20-b499-357695afa875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ab1b89fa-fe60-4989-ab5a-2c84052e53a7\", \"projectname.pickle\", 36)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}